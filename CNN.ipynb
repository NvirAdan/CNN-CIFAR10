{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f44d848e-68af-4781-bedf-f4e6b9ac770a",
   "metadata": {},
   "source": [
    "Import the Dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b554267e-eed9-45b2-9133-ee4578cb2541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms,datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb637ec-2d5f-4e2f-9acd-08b9ac0914a9",
   "metadata": {},
   "source": [
    "Inside the transform.Compose \"pipeline\" we convert the images of Cifar10 which comes in PIL format to pytorch tensors using transforms.ToTensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a797096-260e-4a03-870b-90437ece49ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f9dc7e-ad0b-4727-9176-b0ee7e86ebdf",
   "metadata": {},
   "source": [
    "Download of the (train)CIFAR10 dataset and creation of batches of 64 images(shuffled)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d12972e-bcdf-498f-b728-04ee222029ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.CIFAR10(root=\"./data\",train=True,download=True,transform=transform)\n",
    "train_batch = DataLoader(dataset,batch_size=64,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1718c85e-2234-4747-b343-a8caadb99872",
   "metadata": {},
   "source": [
    "Creation of the function to initialize the parameters W & B of each kernel.\\\n",
    "Here the \"filters\" will be the amount of kernels that we gonna have in each conv2d layer.\\\n",
    "The \"channels\" parameter because we're using kernels that are tensor3d with height,width and depth.The amount of channels will change,for example,the first(before enter the conv2d layer)will be 3(RGB) and then the amount will change representing every channel a feature map(basically is one feature map per kernel of the previous conv2d layer).\\\n",
    "And the kernel_size is how big the kernel in terms of height and width it gonna be,in this model i'll use a 3x3 kernel size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd9117c7-d995-4b3e-bc98-46efd4af4984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_conv2_params(filters,channels,kernel_size):\n",
    "    \n",
    "    fan_in = channels*kernel_size*kernel_size\n",
    "\n",
    "    Wconv = np.random.randn(filters,channels,kernel_size,kernel_size) * np.sqrt(2/fan_in)\n",
    "\n",
    "    Bconv = np.zeros((filters))\n",
    "\n",
    "    return Wconv,Bconv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d682ac-c518-4db2-ad99-7f1e1d71fa33",
   "metadata": {},
   "source": [
    "We initialize the parameters giving the respective arguments.\\\n",
    "I gonna use 4 conv2d layers giving to my CNN an architecture of:\n",
    "\n",
    "Conv2d->ReLU\\\n",
    "Conv2d->ReLU\\\n",
    "Maxpooling\n",
    "\n",
    "Conv2d->ReLU\\\n",
    "Conv2d->ReLU\\\n",
    "Maxpooling\n",
    "\n",
    "Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f5065fb-5842-45a9-a0fe-3485492f66c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1,B1 = init_conv2_params(16,3,3)\n",
    "W2,B2 = init_conv2_params(32,16,3)\n",
    "W3,B3 = init_conv2_params(64,32,3)\n",
    "W4,B4 = init_conv2_params(128,64,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1402ad7b-51fb-4f81-a980-5b7669a80356",
   "metadata": {},
   "source": [
    "This is the function for each Convolutional Layer in this Neural Network.\\\n",
    "Breaking it down:\n",
    "\n",
    "We declare the function setting up the parameters which will be the batch of images(images),the kernels and its respective weights(kernels),the bias of each kernel(Bias) and some fixed parameters: The padding that we gonna give to each image(Padding=1) and the step that each image should do(Stride=1). \n",
    "\n",
    "The first part of the block of code is the if statement,it will detect if the batch of images is a pytorch tensor and if it is detach it from its \"mathematical obligations\" being an pytorch tensor and \"move it\" to the cpu for us,so we can convert it to a numpy tensor.\n",
    "\n",
    "Next we take the shape of the Batch an kernels so we can work with each variable individually.\n",
    "\n",
    "Then make the variables that represent the height and width of each image that gonna leave the function,this being 32x32 (without padding it would be something like 30x30 because it don't reach the borders and shrinks the resolution).\\\n",
    "Comment:the \"+1\" at the final of the line is just because the kernel start counting after it make the first move but the actual first position is BEFORE it moves so the +1 represent the initial position.That's it.\n",
    "\n",
    "The variable \"out\" will be the \"new batch\" that gonna leave the conv2d function.It could be interpretered as an empty box in which we will put the results of each image after going through every kernel in the layer.\n",
    "\n",
    "After all this preparation here is the nested loop:\n",
    "\n",
    "1-This for loop will pass for every image of the batch and it will be padding each one of the images.\\\n",
    "2-This for loop will make the image pass through every filter(kernel).\\\n",
    "3-This for loop is just to travel all the height of the image.\\\n",
    "4-This for loop is have the same goal as the for loop of the height but the important difference here is the order in which we gonna travel through the image,this being first left to right then up to down(for every full pass left to right we move a little bit down and make another full pass).\n",
    "\n",
    "Now the explanation of what we do in every cycle:\n",
    "\n",
    "In these CNNs we usually say that every kernel scans every \"little part\" of the image and call the \"little part\" as \"window\" or \"patch\".\\\n",
    "Well in reality we take the patch of the image and \"Swap it\" through the weights of the kernel(Like a Credit Card),each kernel have its own weights that have the same dimensions than the patch,so it will give us a \"new patch\" this being modifier by the weights and bias of each kernel,so finally when we go through the entire image with the same kernel it will give us a Feature Map,that represent the previous image modified by the kernel,this image will have the same height and width in this case because we've used padding.\n",
    "\n",
    "In the code we have exactly that,the start and end of the height and width of every patch corresponding to the current position of the loop.\\\n",
    "The patch which will take a portion of the image padded of the size previously mentioned.\\\n",
    "And then the storage of the patch \"modified\" by the kernel in our new empty box previously declared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c165a94-1cca-4d4e-b023-46968a9118c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def foward_conv2d(images,kernels,Bias,Padding=1,Stride=1):\n",
    "\n",
    "    if isinstance(images, torch.Tensor):\n",
    "        images = images.detach().cpu().numpy()\n",
    "\n",
    "    batch_size,channel,h_in,w_in = images.shape\n",
    "    filters,channels,kh,kw = kernels.shape\n",
    "\n",
    "    h_out = (h_in - kh + 2*Padding) // Stride + 1\n",
    "    w_out = (w_in - kw + 2*Padding) // Stride + 1\n",
    "\n",
    "    out= np.zeros((batch_size,filters,h_out,w_out))\n",
    "\n",
    "    for x in range(batch_size):\n",
    "        img = images[x]\n",
    "        img_padded = np.pad(img,((0,0),(Padding,Padding),(Padding,Padding)),mode=\"constant\")\n",
    "        for f in range(filters):\n",
    "            for h in range(h_out):\n",
    "                for w in range(w_out):\n",
    "                        \n",
    "                        w_start = w * Stride \n",
    "                        w_end = w_start + kw\n",
    "\n",
    "                        h_start = h * Stride\n",
    "                        h_end = h_start + kh\n",
    "\n",
    "                        patch = img_padded[:,h_start:h_end,w_start:w_end]\n",
    "\n",
    "                        out[x,f,h,w]= np.sum(patch * kernels[f]) + Bias[f]\n",
    "\n",
    "    return out\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2f5603-d728-4ddd-9e8b-7443c3ee1929",
   "metadata": {},
   "source": [
    "Classic ReLU to make the negative values \"turn off\" so we only work with the positive values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "418ab289-a15a-42d4-bbb9-fc40d8688c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(Output):\n",
    "    return np.maximum(0,Output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9139bab8-cdb1-4692-ae35-dbca67e6aae8",
   "metadata": {},
   "source": [
    "Here i'ma apply a maxpooling function,this will help us to use less computational power in the long run, simplifying the resolution of each image by exactly 50%.\\\n",
    "In terms of coding is basically the same as the conv2d layer,but it has some functional changes:\n",
    "\n",
    "Kelnel :In comparation with the kernel of the convolutional layer this kernel doesn't have any weight or bias,it's only function is to detect(in the quadrant that we select,in this case 2x2 pixels) the highest value and return a 1x1 with that number,eliminating the others smaller values in the quadrant selected.\n",
    "\n",
    "\n",
    "Stride :Previously in the conv2d layer we've used an Stride of 1.In this case the Stride will be of 2,this combining with the new size and purpose given to the kernel makes that we can get the desire output size of the image after going through maxpooling.\n",
    "\n",
    "Mask :This is required only for the back propagation of the convolutional layers.Is a tensor4d with the same size of the Input of the function but instead of Numerical Values we use Boolean Values,we need to keep this information when we do the foward pass because it will tell us in the moment of the back propagation where the max_value was in order to pass the gradient to the correct \"spot\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "878b61c9-6414-4cd7-874e-ff8800d38a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxpooling(Input,pool_h=2,pool_w=2,Stride=2):\n",
    "\n",
    "    batch_size,channels,h_in,w_in = Input.shape\n",
    "\n",
    "    h_out = (h_in - pool_h) // Stride + 1\n",
    "    w_out = (w_in - pool_w) // Stride + 1\n",
    "\n",
    "    out = np.zeros((batch_size,channels,h_out,w_out))\n",
    "\n",
    "    mask = np.zeros_like(Input, dtype=bool)\n",
    "\n",
    "    for x in range(batch_size):\n",
    "        for c in range (channels):\n",
    "            for h in range (h_out):\n",
    "                for w in range(w_out):\n",
    "\n",
    "                    h_start = h * Stride\n",
    "                    h_end = h_start + pool_h\n",
    "\n",
    "                    w_start = w * Stride\n",
    "                    w_end = w_start + pool_w\n",
    "\n",
    "                    patch = Input[x,c,h_start:h_end,w_start:w_end]\n",
    "\n",
    "                    max_value = np.max(patch)\n",
    "                    out[x,c,h,w] = max_value\n",
    "\n",
    "                    mask[x,c,h_start:h_end,w_start:w_end] += (patch == max_value)\n",
    "    return out,mask\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79d9bc9-efc7-4b53-a708-ae517c3c90d9",
   "metadata": {},
   "source": [
    "Normally inside the Foward Pass of a CNN you can see 2 separated \"regions\".\\\n",
    "The first one being the \"region\" where the convolution happens,the convolutional layers, And the other \"region\" that is in almost every classification model you can find, the FC(fully-connected) layers.\n",
    "\n",
    "Now what this matter in the explanation of this function?\n",
    "\n",
    "Because we can see this function like a gate or portal that transforms a tensor into another tensor.\\\n",
    "At first in the convolutional part we work with tensors4d (batch_size,channels,height,width),now after the flatten we have got a tensor1d(64,128 * 8 * 8)because with the flatten we collapse the dimensions,this being exactly what we needed for the fc layers to work.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7b62374-7b86-40d4-a6cf-b9071f15ee88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(tensor_in):\n",
    "    flatten_tensor = tensor_in.reshape(tensor_in.shape[0],-1)\n",
    "    return flatten_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcd53ee-3db3-4acb-af81-327722d0e793",
   "metadata": {},
   "source": [
    "Initialization of the FC layers parameters,the change in the function with respect to the initialization of the conv2d parameters is in the dimensions.\\\n",
    "This function creating weights as a tensor2d and every neuron getting it's own personal bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be5f24e1-5115-40be-ada0-d57501f3a8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_fc_params(neurons,flatten):\n",
    "\n",
    "    Wfc = np.random.randn(flatten,neurons) * np.sqrt(2/flatten)\n",
    "\n",
    "    Bfc = np.zeros((neurons))\n",
    "\n",
    "    return Wfc,Bfc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df28e7c6-1910-460c-8b83-ee88f8f0ac45",
   "metadata": {},
   "source": [
    "For us to initialize the parameters of the FC layers we need to know the dimensions of the flatten in order to pass it to the init_fc_params function.\\\n",
    "For that reason we use a \"batch dummy\"(which is a tensor imitating an image of cifar10) and pass it through the convolutional part of our CNN,at the end we just need to measure the shape of the final result.\\\n",
    "We use this to make a more modular CNN model,because if we use a different dataset or training method the dimensions may change.\\\n",
    "So in order to prevent that we just can run this cell with the respective size of \"batch dummy\" and always know the dimension that we would get when we collapse it to flatten it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d63c865-befd-4c1f-904c-edf7f1d321d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 128, 8, 8)\n",
      "8192\n"
     ]
    }
   ],
   "source": [
    "flatten_dim = np.zeros((1,3,32,32))\n",
    "flatten_dim = foward_conv2d(flatten_dim,W1,B1)\n",
    "flatten_dim = foward_conv2d(flatten_dim,W2,B2)\n",
    "flatten_dim = ReLU(flatten_dim)\n",
    "flatten_dim = maxpooling(flatten_dim)\n",
    "\n",
    "flatten_dim = foward_conv2d(flatten_dim,W3,B3)\n",
    "flatten_dim = foward_conv2d(flatten_dim,W4,B4)\n",
    "flatten_dim = ReLU(flatten_dim)\n",
    "flatten_dim = maxpooling(flatten_dim)\n",
    "\n",
    "print(flatten_dim.shape)\n",
    "\n",
    "flatten_size = np.prod(flatten_dim.shape[1:])\n",
    "print(flatten_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bf0f04-e55f-45ae-8b26-8d79b8a4ae04",
   "metadata": {},
   "source": [
    "Now we can call the function to initilize our parameters of the FC layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ecb38b0-9383-47c3-8e0e-068dad51aa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "W5,B5 = init_fc_params(128,flatten_size) \n",
    "W6,B6 = init_fc_params(10,128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc6cdee-a2f0-4588-99c6-66810ba1e168",
   "metadata": {},
   "source": [
    "This is the Softmax function to get the final prediction of our model giving to this funcion the Z6 that we got in the final sum of our foward pass,for a more formal explanation I encourage you to check the NN from scratch using the dataset of MNIST in my github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c279f4a-df29-4925-8986-c8271407f1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Softmax(Z2):\n",
    "\n",
    "    shifted = Z2 - np.max(Z2,axis=1,keepdims=True)\n",
    "\n",
    "    scores = np.exp(shifted)\n",
    "\n",
    "    preds = scores / np.sum(scores,axis=1,keepdims=True)\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9139e5e-12e3-4f1d-bace-06fed8b588d8",
   "metadata": {},
   "source": [
    "This is the other \"region\" that I talk about earlier, is the most common part of the Neural Networks.\\\n",
    "We take the flatten version of our batch which will have a dimension of (64,8192) and multiply them by the weights of dimensions(8192,128) at the end we just sum the bias corresponding to every neuron so we can get a Z5 of dimensions(64,128) to then giving them to the ReLU function to get the activation of everyone(A5).\\\n",
    "Now we do the same process but instead of using flatten we use the A5(64,128) and multiply it by the weights(128,10),sum them with the bias of each neuron and getting the Z6(64,10).\\\n",
    "Then Just pass it to the Softmax function and getting the preddiction of the model as A6.\\\n",
    "After that we return Z5,A5,Z6,A6 because we will need it to calculate the back propagation and update de parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "daf7f4a1-4d87-4265-b011-99c39d351dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def foward_fc(W5,B5,W6,B6,flatten):\n",
    "\n",
    "    Z5 = flatten @ W5 + B5\n",
    "    \n",
    "    A5 = ReLU(Z5)\n",
    "\n",
    "    Z6 = A5 @ W6 + B6\n",
    "\n",
    "    A6 = Softmax(Z6)\n",
    "\n",
    "    return Z5,A5,Z6,A6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8609a357-7349-4fe3-a341-350367a8711f",
   "metadata": {},
   "source": [
    "I need to clarify that this function is not extrictly needed as it doesn't has any influence in the training of the model.\\\n",
    "But is important because if we don't have it we gonna be training the model blind.Without this function we don't have access to know how good is the model improving and we can't know when to stop it,playing with the risk of overfitting.\\\n",
    "So this fuction is like the control panel of the model.\n",
    "\n",
    "In the function we have if statements,for us to be sure that we're working with numpy tensors.\\\n",
    "Then a variable to know the batch size (64 in this case,we don't hardcode \"64\" in order to make the function modular).\\\n",
    "Now the correct_answers variable contains the values of the correct class according to the labels.\\\n",
    "After that we declared an special number because in the next step we gonna declare the \"logs\" variable that gets the -log values of the correct_ans variable and we need to add this special number.It will help to maintain a certain minimum value,because computers tends to take as a 0 the values that go too low and that is also a thing we don't want.\\\n",
    "At the end we just return the mean value of the loss of the entire batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1a100f4-5408-4571-aff8-2402e2ba01ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(predictions,labels):\n",
    "\n",
    "    if isinstance(predictions, torch.Tensor):\n",
    "        predictions = predictions.detach().cpu().numpy()\n",
    "        \n",
    "    if isinstance(labels, torch.Tensor):\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "\n",
    "    batch_size = predictions.shape[0]\n",
    "\n",
    "    correct_ans = predictions[np.arange(batch_size),labels]\n",
    "\n",
    "    spe_num=1e-12\n",
    "\n",
    "    logs = -np.log(correct_ans + spe_num)\n",
    "\n",
    "    return np.mean(logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa698ef5-b656-44ca-889a-fd596fefb27b",
   "metadata": {},
   "source": [
    "This function is the backpropagation of the FC layers which will give us the gradient of every parameter in the fc layer according to the true prediction that it should give us.\n",
    "\n",
    "First we take the predictions and make a copy of it,so we don't mess the model's predictions and we can keep working.\\\n",
    "Second we compare the prediction with the onehot encoding of the true values,previously in the NN of MNIST I have made a function to get the onehot encoding,but this is a more professional way of doing it(no new functions,only one line of code,using the same trick of the advance indexing of numpy,also used in the loss function).\n",
    "\n",
    "Now that we have the dZ6 which is of much of error the predictions of the model has we can do back propagation to get each parameter:\n",
    "\n",
    "dW6 = Is the diference between A5 and Z6 of the foward pass(because of the weights in that part),so we got A5 transposed(64,128 -> 128,64) and dZ6(64,10),the dot product of that will be how much of influence had the weights there.\n",
    "\n",
    "dB6 = Is just the summatory of all the errors of each image of the batch in that layer.\n",
    "\n",
    "dA5 = Is the derivative of the activation of the hidden layer so in order to get the derivative of the weights and bias of W5 and B5 we need to know how A5 has contributed in the foward pass,the formula to get it is just following the chain rule.getting the dot product of dZ6(64,10)by the W6 transposed(128,10 -> 10,128) = dA5 = (64,128)\n",
    "\n",
    "dReLU = This is the derivative of ReLU which we need to \"cut\" the backpropagation(stop the backpropagation where ReLU hasn't activated),it is just a matrix of the size of Z5 full of booleans,then transforms those booleans into float numbers.\n",
    "\n",
    "dZ5 = This is the derivative of Z5 which is the multiplication(by element,for that reason is the \" * \" instead of the \" @ \")of dA5 by dReLU,so dZ5 is basically dA5 (64,128) but thanks to dReLU some values are multiplied by 0 (so the values turns 0 stopping the back propagation there) and the values multiplied by 1 still intact. \n",
    "\n",
    "Now that we got dZ5 we can do the same process with the next layer:\n",
    "\n",
    "dW5 = Is now the dot product of the flatten transposed(64,8192 -> 8192,64) by dZ5 (64,128) so dW5 is (8192,128)exactly the amount of weights of that layer.\n",
    "\n",
    "dB5 = And this is again just a summatory of the errors in that layer.\n",
    "\n",
    "dflatten = The final thing that we gonna do in the backpropagation of the FC layers is the derivative of the flatten,which is our input in the FC but just a connection between conv2d and fc layers.To get it is the same thing as the dA5, but instead of dZ6 and W6.T is dZ5 and W5.T."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ee33b04-c9bf-4445-ad29-152702a82d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Back_propagation(predictions,A5,Z5,W6,W5,flatten):\n",
    "\n",
    "    # Making a copy so we don't break the results of the foward pass.\n",
    "    dZ6 = predictions.copy() \n",
    "\n",
    "    # Softmax of the predictions - Onehot_encoding of the labels using the advance indexing of numpy.\n",
    "    dZ6[np.arange(len(labels)),labels] -= 1\n",
    "\n",
    "    dW6 = A5.T @ dZ6\n",
    "\n",
    "    dB6 = np.sum(dZ6,axis = 0)\n",
    "\n",
    "    dA5 = dZ6 @ W6.T\n",
    "    \n",
    "    dReLU = (Z5 > 0).astype(float)\n",
    "    \n",
    "    dZ5 = dA5 * dReLU\n",
    "\n",
    "    dW5 = flatten.T @ dZ5\n",
    "\n",
    "    dB5 = np.sum(dZ5,axis = 0)\n",
    "\n",
    "    dflatten = dZ5 @ W5.T\n",
    "    \n",
    "    return dZ6,dW6,dB6,dA5,dZ5,dW5,dB5,dflatten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd6b0f7-23b4-4be2-ae02-62e5259b2a79",
   "metadata": {},
   "source": [
    "Now that we got the derivative of the input of the FC layers we need to \"unflattenize\" it.\\\n",
    "The dflatten has the dimensions (64,8192) but we need the dimensions previous of the flatten function when we did the foward pass (64,128,8,8) in order for us to use it so we can keep calculating the gradients in the convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66c5eee4-57fc-40c7-9cb1-de705f68c1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unflattenize(dflatten,before_flatten_dim):\n",
    "\n",
    "    dA_maxpooling = dflatten.reshape(before_flatten_dim)\n",
    "\n",
    "    return dA_maxpooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49538abb-670c-401e-8c59-55d73cd4edde",
   "metadata": {},
   "source": [
    "This is the back propagation of the maxpooling.\\\n",
    "Here we assign as parameter the derivative of the input layer of the FC layers with the dimensions previous of the flatten,the mask that we have saved in the foward pass and some fixed parameters that are the same as the maxpooling function of the foward pass.\n",
    "\n",
    "First we take the shape of the dinput_unflattenized(also called \"dout\",but I'll use dinput_unflattenized for me to be easier to understand).\\\n",
    "Second we need to create a new tensor4d with the same size and dimensions as the mask and filll it out with float numbers(0.0).\\\n",
    "Then we have another nested loop, mostly when we have a nested loop in the CNNs it's just to do 3 things,take a \"patch/window\" every loop,do something with it (for example in the maxpooling of the foward pass we take the max_value of the patch) and then \"save\" that information in the tensor previously created.\\\n",
    "Here we declare the current gradient of the loop as a variable \"current_gradient\".\\\n",
    "Then take the current patch of the mask and multiply it by the current gradient,because the patch that we take is a 2x2 with 3 falses and 1 true(the position of the max_value before maxpooling being the true) and because is boolean it will represent it's values as 1 and 0,so when we multiply the gradient with the falses(0) those positions will have a 0 on it and the position of the true(1) will have the gradient,with that we have the gradient in the correct position and we can save it in the tensor4d of floats previously declared,returning that tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54ff3123-bb3a-4824-9e29-b3a0b3dff7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop_maxpooling(dinput_unflattenized,mask,h_pool=2,w_pool=2,stride=2):\n",
    "\n",
    "    batch_size,channels,h_out,w_out = dinput_unflattenized.shape\n",
    "\n",
    "    dInput = np.zeros_like(mask, dtype=float)\n",
    "\n",
    "    for x in range(batch_size):\n",
    "        for c in range(channels):\n",
    "            for h in range(h_out):\n",
    "                for w in range(w_out):\n",
    "\n",
    "                    h_start = h * stride\n",
    "                    h_end = h_start + h_pool\n",
    "\n",
    "                    w_start = w * stride\n",
    "                    w_end = w_start + w_pool\n",
    "\n",
    "                    current_gradient = dinput_unflattenized[x,c,h,w]\n",
    "\n",
    "                    dInput[x,c,h_start:h_end,w_start:w_end] += mask[x,c,h_start:h_end,w_start:w_end] * current_gradient\n",
    "\n",
    "    return dInput\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c032fc-f1b7-40a4-8314-8a7f70d49ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
