{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f44d848e-68af-4781-bedf-f4e6b9ac770a",
   "metadata": {},
   "source": [
    "Import the Dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b554267e-eed9-45b2-9133-ee4578cb2541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms,datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb637ec-2d5f-4e2f-9acd-08b9ac0914a9",
   "metadata": {},
   "source": [
    "Inside the transform.Compose \"pipeline\" we convert the images of Cifar10 which comes in PIL format to pytorch tensors using transforms.ToTensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a797096-260e-4a03-870b-90437ece49ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f9dc7e-ad0b-4727-9176-b0ee7e86ebdf",
   "metadata": {},
   "source": [
    "Download of the (train)CIFAR10 dataset and creation of batches of 64 images(shuffled)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d12972e-bcdf-498f-b728-04ee222029ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.CIFAR10(root=\"./data\",train=True,download=True,transform=transform)\n",
    "train_batch = DataLoader(dataset,batch_size=64,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1718c85e-2234-4747-b343-a8caadb99872",
   "metadata": {},
   "source": [
    "Creation of the function to initialize the parameters W & B of each kernel.\\\n",
    "Here the \"filters\" will be the amount of kernels that we gonna have in each conv2d layer.\\\n",
    "The \"channels\" parameter because we're using kernels that are tensor3d with height,width and depth.The amount of channels will change,for example,the first(before enter the conv2d layer)will be 3(RGB) and then the amount will change representing every channel a feature map(basically is one feature map per kernel of the previous conv2d layer).\\\n",
    "And the kernel_size is how big the kernel in terms of height and width it gonna be,in this model i'll use a 3x3 kernel size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd9117c7-d995-4b3e-bc98-46efd4af4984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_conv2_params(filters,channels,kernel_size):\n",
    "    \n",
    "    fan_in = channels*kernel_size*kernel_size\n",
    "\n",
    "    Wconv = np.random.randn(filters,channels,kernel_size,kernel_size) * np.sqrt(2/fan_in)\n",
    "\n",
    "    Bconv = np.zeros((filters))\n",
    "\n",
    "    return Wconv,Bconv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d682ac-c518-4db2-ad99-7f1e1d71fa33",
   "metadata": {},
   "source": [
    "We initialize the parameters giving the respective arguments.\\\n",
    "I gonna use 4 conv2d layers giving to my CNN an architecture of:\n",
    "\n",
    "Conv2d->ReLU\\\n",
    "Conv2d->ReLU\\\n",
    "Maxpooling\n",
    "\n",
    "Conv2d->ReLU\\\n",
    "Conv2d->ReLU\\\n",
    "Maxpooling\n",
    "\n",
    "Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f5065fb-5842-45a9-a0fe-3485492f66c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1,B1 = init_conv2_params(16,3,3)\n",
    "W2,B2 = init_conv2_params(32,16,3)\n",
    "W3,B3 = init_conv2_params(64,32,3)\n",
    "W4,B4 = init_conv2_params(128,64,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1402ad7b-51fb-4f81-a980-5b7669a80356",
   "metadata": {},
   "source": [
    "This is the function for each Convolutional Layer in this Neural Network.\\\n",
    "Breaking it down:\n",
    "\n",
    "We declare the function setting up the parameters which will be the batch of images(images),the kernels and its respective weights(kernels),the bias of each kernel(Bias) and some fixed parameters: The padding that we gonna give to each image(Padding=1) and the step that each image should do(Stride=1). \n",
    "\n",
    "The first part of the block of code is the if statement,it will detect if the batch of images is a pytorch tensor and if it is detach it from its \"mathematical obligations\" being an pytorch tensor and \"move it\" to the cpu for us,so we can convert it to a numpy tensor.\n",
    "\n",
    "Next we take the shape of the Batch an kernels so we can work with each variable individually.\n",
    "\n",
    "Then make the variables that represent the height and width of each image that gonna leave the function,this being 32x32 (without padding it would be something like 30x30 because it don't reach the borders and shrinks the resolution).\\\n",
    "Comment:the \"+1\" at the final of the line is just because the kernel start counting after it make the first move but the actual first position is BEFORE it moves so the +1 represent the initial position.That's it.\n",
    "\n",
    "The variable \"out\" will be the \"new batch\" that gonna leave the conv2d function.It could be interpretered as an empty box in which we will put the results of each image after going through every kernel in the layer.\n",
    "\n",
    "After all this preparation here is the nested loop:\n",
    "\n",
    "1-This for loop will pass for every image of the batch and it will be padding each one of the images.\\\n",
    "2-This for loop will make the image pass through every filter(kernel).\\\n",
    "3-This for loop is just to travel all the height of the image.\\\n",
    "4-This for loop is have the same goal as the for loop of the height but the important difference here is the order in which we gonna travel through the image,this being first left to right then up to down(for every full pass left to right we move a little bit down and make another full pass).\n",
    "\n",
    "Now the explanation of what we do in every cycle:\n",
    "\n",
    "In these CNNs we usually say that every kernel scans every \"little part\" of the image and call the \"little part\" as \"window\" or \"patch\".\\\n",
    "Well in reality we take the patch of the image and \"Swap it\" through the weights of the kernel(Like a Credit Card),each kernel have its own weights that have the same dimensions than the patch,so it will give us a \"new patch\" this being modifier by the weights and bias of each kernel,so finally when we go through the entire image with the same kernel it will give us a Feature Map,that represent the previous image modified by the kernel,this image will have the same height and width in this case because we've used padding.\n",
    "\n",
    "In the code we have exactly that,the start and end of the height and width of every patch corresponding to the current position of the loop.\\\n",
    "The patch which will take a portion of the image padded of the size previously mentioned.\\\n",
    "And then the storage of the patch \"modified\" by the kernel in our new empty box previously declared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c165a94-1cca-4d4e-b023-46968a9118c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def foward_conv2d(images,kernels,Bias,Padding=1,Stride=1):\n",
    "\n",
    "    if isinstance(images, torch.Tensor):\n",
    "        images = images.detach().cpu().numpy()\n",
    "\n",
    "    batch_size,channel,h_in,w_in = images.shape\n",
    "    filters,channels,kh,kw = kernels.shape\n",
    "\n",
    "    h_out = (h_in - kh + 2*Padding) // Stride + 1\n",
    "    w_out = (w_in - kw + 2*Padding) // Stride + 1\n",
    "\n",
    "    out= np.zeros((batch_size,filters,h_out,w_out))\n",
    "\n",
    "    for x in range(batch_size):\n",
    "        img = images[x]\n",
    "        img_padded = np.pad(img,((0,0),(Padding,Padding),(Padding,Padding)),mode=\"constant\")\n",
    "        for f in range(filters):\n",
    "            for h in range(h_out):\n",
    "                for w in range(w_out):\n",
    "                        \n",
    "                        w_start = w * Stride \n",
    "                        w_end = w_start + kw\n",
    "\n",
    "                        h_start = h * Stride\n",
    "                        h_end = h_start + kh\n",
    "\n",
    "                        patch = img_padded[:,h_start:h_end,w_start:w_end]\n",
    "\n",
    "                        out[x,f,h,w]= np.sum(patch * kernels[f]) + Bias[f]\n",
    "\n",
    "    return out\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2f5603-d728-4ddd-9e8b-7443c3ee1929",
   "metadata": {},
   "source": [
    "Classic ReLU to make the negative values \"turn off\" so we only work with the positive values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "418ab289-a15a-42d4-bbb9-fc40d8688c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(Output):\n",
    "    return np.maximum(0,Output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9139bab8-cdb1-4692-ae35-dbca67e6aae8",
   "metadata": {},
   "source": [
    "Here i'ma apply a maxpooling function,this will help us to use less computational power in the long run, simplifying the resolution of each image by exactly 50%.\\\n",
    "In terms of coding is basically the same as the conv2d layer,but it has some functional changes:\n",
    "\n",
    "Kelnel :In comparation with the kernel of the convolutional layer this kernel doesn't have any weight or bias,it's only function is to detect(in the quadrant that we select,in this case 2x2 pixels) the highest value and return a 1x1 with that number,eliminating the others smaller values in the quadrant selected.\n",
    "\n",
    "\n",
    "Stride :Previously in the conv2d layer we've used an Stride of 1.In this case the Stride will be of 2,this combining with the new size and purpose given to the kernel makes that we can get the desire output size of the image after going through maxpooling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "878b61c9-6414-4cd7-874e-ff8800d38a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxpooling(Input,pool_h=2,pool_w=2,Stride=2):\n",
    "\n",
    "    batch_size,channels,h_in,w_in = Input.shape\n",
    "\n",
    "    h_out = (h_in - pool_h) // Stride + 1\n",
    "    w_out = (w_in - pool_w) // Stride + 1\n",
    "\n",
    "    out = np.zeros((batch_size,channels,h_out,w_out))\n",
    "\n",
    "    for x in range(batch_size):\n",
    "        for c in range (channels):\n",
    "            for h in range (h_out):\n",
    "                for w in range(w_out):\n",
    "\n",
    "                    h_start = h * Stride\n",
    "                    h_end = h_start + pool_h\n",
    "\n",
    "                    w_start = w * Stride\n",
    "                    w_end = w_start + pool_w\n",
    "\n",
    "                    patch = Input[x,c,h_start:h_end,w_start:w_end]\n",
    "\n",
    "                    out[x,c,h,w] = np.max(patch)\n",
    "\n",
    "    return out\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79d9bc9-efc7-4b53-a708-ae517c3c90d9",
   "metadata": {},
   "source": [
    "Normally inside the Foward Pass of a CNN you can see 2 separated \"regions\".\\\n",
    "The first one being the \"region\" where the convolution happens,the convolutional layers, And the other \"region\" that is in almost every classification model you can find, the FC(fully-connected) layers.\n",
    "\n",
    "Now what this matter in the explanation of this function?\n",
    "\n",
    "Because we can see this function like a gate or portal that transforms a tensor into another tensor.\\\n",
    "At first in the convolutional part we work with tensors4d (batch_size,channels,height,width),now after the flatten we have got a tensor1d(64,128 * 8 * 8)because with the flatten we collapse the dimensions,this being exactly what we needed for the fc layers to work.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b62374-7b86-40d4-a6cf-b9071f15ee88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(tensor_in):\n",
    "    flatten_tensor = tensor_in.reshape(tensor_in.shape[0],-1)\n",
    "    return flatten_tensor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
